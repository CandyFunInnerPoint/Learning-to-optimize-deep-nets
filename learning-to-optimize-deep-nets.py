# -*- coding: utf-8 -*-
"""Copy of assesment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EiQlz9YaXVifpJOhiHj7ILpMZpo1DEny
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import glob
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.autograd as autograd
import torch.optim as optim
from torch.autograd import Variable
from torch.distributions.normal import Normal
import matplotlib.pyplot as plt
import random
from tqdm import tqdm_notebook as tqdm
import multiprocessing
import os.path
import csv
import copy
import joblib
from collections import deque
from torchvision import datasets
import torchvision
import seaborn as sns; sns.set(color_codes=True)
sns.set_style("white")

USE_CUDA = True

def w(v):
    if USE_CUDA:
        return v.cuda()
    return v

def detach_var(v):
    var = w(Variable(v.data, requires_grad=True))
    var.retain_grad()
    return var

def preprocess(inp, p):
    # takes in input of size [parameters x 1]
    inp = inp.data
    n_param = inp.size()[0] 
    dim = inp.size()[1]
    inp2 = w(torch.zeros(n_param, 2*dim))
    keep_grads = torch.abs(inp) >= np.exp(-p)

    inp2[:, 0:dim][keep_grads] = torch.log(torch.abs(inp[keep_grads]) + 1e-8) / p
    inp2[:, dim:][keep_grads] = torch.sign(inp[keep_grads])

    inp2[:, 0:dim][~keep_grads] = -1
    inp2[:, dim:][~keep_grads] = float(np.exp(p)) * inp[~keep_grads]

    return w(Variable(inp2))

USE_CUDA = True

def w(v):
    if USE_CUDA:
        return v.cuda()
    return v

def do_fit(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, out_mul, phase='train'):   
    #print(locals())
    if phase == 'train':
        should_train = True
        opt_net.train()
    else:
        should_train = False
        opt_net.eval()
        unroll = 1
    
    # load dataset
    target = target_cls(phase=phase)
    # load optimizee
    optimizee = w(target_to_opt())

    # collect all parameters
    n_params = 0
    for p in optimizee.parameters():
        n_params += int(np.prod(p.size()))

    # create hidden states for RNN
    hidden_states = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]
    cell_states = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]

    
    all_losses_ever = []
    if should_train:
        meta_opt.zero_grad()
    all_losses = None
    for iteration in range(1, optim_it + 1):
        loss = optimizee(target)

        if all_losses is None:
            all_losses = loss
        else:
            all_losses += loss

        all_losses_ever.append(loss.data.cpu().numpy())
        loss.backward(retain_graph=should_train)

        offset = 0
        optimizee_params = {}
        hidden_states2 = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]
        cell_states2 = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]
        
        # for each parameter we disconnect it from the graph and get the gradient 
        # to use as the inputs to the optimizer 
        for name, p in optimizee.all_named_parameters():
            cur_size = int(np.prod(p.size()))

            gradients = detach_var(p.grad.view(cur_size, 1))
            # get updates for the parameters and hidden state of the lstm
            updates, new_hidden, new_cell = opt_net(
                gradients,
                [h[offset:offset+cur_size] for h in hidden_states],
                [c[offset:offset+cur_size] for c in cell_states]
            )
            for i in range(len(new_hidden)):
                hidden_states2[i][offset:offset+cur_size] = new_hidden[i]
                cell_states2[i][offset:offset+cur_size] = new_cell[i]
            optimizee_params[name] = p + updates.view(*p.size()) * out_mul
            optimizee_params[name].retain_grad()
        
        # after unrolling do the optimizer step
        if iteration % unroll == 0:
            if should_train:
                meta_opt.zero_grad()
                all_losses.backward()
                meta_opt.step()

            all_losses = None

            optimizee = w(target_to_opt(**{k: detach_var(v) for k, v in optimizee_params.items()}))
            hidden_states = [detach_var(v) for v in hidden_states2]
            cell_states = [detach_var(v) for v in cell_states2]
            
        else:
            optimizee = w(target_to_opt(**optimizee_params))
            assert len(list(optimizee.all_named_parameters()))
            hidden_states = hidden_states2
            cell_states = cell_states2
            
    return all_losses_ever

def fit_optimizer(target_cls, target_to_opt, preproc=True, unroll=20, optim_it=100, n_epochs=20, n_tests=100, lr=0.001, out_mul=1.0):
    opt_net = w(RNN_Optimizer(preproc=preproc))
    meta_opt = optim.Adam(opt_net.parameters(), lr=lr)
    
    best_net = None
    best_loss = 100000000000000000
    
    for _ in tqdm(range(n_epochs), 'epochs'):
        for _ in tqdm(range(20), 'iterations'):
            do_fit(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, out_mul, phase='train')
        
        loss = (np.mean([
            np.sum(do_fit(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, out_mul, phase='val'))
            for _ in tqdm(range(n_tests), 'tests')
        ]))
        print(loss)
        if loss < best_loss:
            print(best_loss, loss)
            best_loss = loss
            best_net = copy.deepcopy(opt_net.state_dict())
            
    return best_loss, best_net

"""policy mlp"""

VARIANCE = w(torch.tensor([0.03]))
N_PREV_GRADS = 5

def do_fit_PG(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, out_mul, phase='train'):
    if phase == 'train':
        should_train = True
        opt_net.train()
    else:
        should_train = False
        opt_net.eval()
        unroll = 1

    # load dataset
    target = target_cls(phase=phase)
    # load optimizee
    optimizee = w(target_to_opt())

    # collect all parameters
    n_params = 0
    for p in optimizee.parameters():
        n_params += int(np.prod(p.size()))
        
    m = Normal(0.0, VARIANCE)

    all_losses_ever = []
    if should_train:
        meta_opt.zero_grad()
    
    all_losses = []
    saved_log_probs = []
    
    all_gradients = dict()
    for name, p in optimizee.all_named_parameters():
        all_gradients[name] = deque(maxlen=N_PREV_GRADS)
        for _ in range(N_PREV_GRADS):
            all_gradients[name].append(torch.reshape(torch.zeros_like(p), (-1,1)))
    
    for iteration in range(1, optim_it + 1):
        loss = optimizee(target)

        all_losses.append(loss)

        all_losses_ever.append(loss.data.cpu().numpy())
        loss.backward(retain_graph=should_train)
                
        optimizee_params = {}

        # for each parameter we disconnect it from the graph and get the gradient
        # to use as the inputs to the optimizer
        for name, p in optimizee.all_named_parameters():
            cur_size = int(np.prod(p.size()))

            gradients = detach_var(p.grad.view(cur_size, 1))
            
            all_gradients[name].append(gradients)
            #print(name, all_gradients[name], all_gradients[name][0].shape, all_gradients[name][-1].shape)
            processed_grads = torch.stack(list(all_gradients[name]), dim=1).squeeze(-1)
                        
            # get updates for the parameters and hidden state of the lstm
            updates = opt_net(
                processed_grads
            )

            if should_train:
                noise = w(m.sample(updates.size()))
                # since the covariance matrix is diagonal, 
                # we can sum up the log probabilities of each dimension
                saved_log_probs.append(m.log_prob(noise).sum()) 

                updates = updates + noise.view(updates.size())
                        
            optimizee_params[name] = p + updates.view(*p.size()) * out_mul
            optimizee_params[name].retain_grad()

        # after unrolling do the optimizer step
        if iteration % unroll == 0:
            if should_train:
                policy_losses = []

                for log_prob, loss in zip(saved_log_probs, all_losses):
                    policy_losses.append(log_prob * loss)
                
                meta_opt.zero_grad()
                policy_losses = torch.stack(policy_losses, dim=0).sum()
                policy_losses.backward()
                
                meta_opt.step()

            all_losses = []
            saved_log_probs = []

            optimizee = w(target_to_opt(
                **{k: detach_var(v) for k, v in optimizee_params.items()}))

        else:
            optimizee = w(target_to_opt(**optimizee_params))
            assert len(list(optimizee.all_named_parameters()))
            
    return all_losses_ever

def fit_optimizer_PG(target_cls, target_to_opt, unroll=20, optim_it=100, n_epochs=20, n_tests=100, lr=0.001, out_mul=1.0, **kwargs):
    opt_net = w(PG_Optimizer(**kwargs))
    meta_opt = optim.Adam(opt_net.parameters(), lr=lr)

    best_net = None
    best_loss = 1e17

    for _ in tqdm(range(n_epochs), 'epochs'):
        for _ in tqdm(range(20), 'iterations'):
            do_fit_PG(opt_net, meta_opt, target_cls, target_to_opt,
                      unroll, optim_it, out_mul, phase='train')

        loss = (np.mean([
            np.sum(do_fit_PG(opt_net, meta_opt, target_cls, target_to_opt,
                             unroll, optim_it, out_mul, phase='val'))
            for _ in tqdm(range(n_tests), 'tests')
        ]))
        print(loss)
        if loss < best_loss:
            print(best_loss, loss)
            best_loss = loss
            best_net = copy.deepcopy(opt_net.state_dict())

    return best_loss, best_net

"""PG AC

"""

def do_fit_AC(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, out_mul, phase='train'):
    if phase == 'train':
        should_train = True
        opt_net.train()
    else:
        should_train = False
        opt_net.eval()
        unroll = 1

    # load dataset
    target = target_cls(phase=phase)
    # load optimizee
    optimizee = w(target_to_opt())

    # collect all parameters
    n_params = 0
    for p in optimizee.parameters():
        n_params += int(np.prod(p.size()))
        
    m = Normal(0.0, VARIANCE)

    all_losses_ever = []
    if should_train:
        meta_opt.zero_grad()
    
    all_losses = []
    saved_log_probs = []
    saved_state_values = []
    
    all_gradients = dict()
    for name, p in optimizee.all_named_parameters():
        all_gradients[name] = deque(maxlen=N_PREV_GRADS)
        for _ in range(N_PREV_GRADS):
            all_gradients[name].append(torch.reshape(torch.zeros_like(p), (-1,1)))
    
    for iteration in range(1, optim_it + 1):
        loss = optimizee(target)

        all_losses.append(loss)

        all_losses_ever.append(loss.data.cpu().numpy())
        loss.backward(retain_graph=should_train)
                
        optimizee_params = {}

        # for each parameter we disconnect it from the graph and get the gradient
        # to use as the inputs to the optimizer
        for name, p in optimizee.all_named_parameters():
            cur_size = int(np.prod(p.size()))

            gradients = detach_var(p.grad.view(cur_size, 1))
            
            all_gradients[name].append(gradients)
            #print(name, all_gradients[name], all_gradients[name][0].shape, all_gradients[name][-1].shape)
            processed_grads = torch.stack(list(all_gradients[name]), dim=1).squeeze(-1)
                        
            # get updates for the parameters and hidden state of the lstm
            updates, state_value = opt_net(
                processed_grads
            )

            if should_train:
                noise = w(m.sample(updates.size()))

                saved_log_probs.append(m.log_prob(noise)) 
                saved_state_values.append(state_value)
                
                updates = updates + noise.view(updates.size())
                        
            optimizee_params[name] = p + updates.view(*p.size()) * out_mul
            optimizee_params[name].retain_grad()

        # after unrolling do the optimizer step
        if iteration % unroll == 0:
            if should_train:
                policy_losses = []
                value_losses = []
                for log_prob, value, loss in zip(saved_log_probs, saved_state_values, all_losses):
                    loss = w(torch.full_like(value, loss.item()))
                    loss_with_baseline = loss - value
                    policy_losses.append(log_prob * loss_with_baseline)
                    value_losses.append(F.smooth_l1_loss(value, loss))
                
                meta_opt.zero_grad()
                losses = torch.stack(policy_losses, dim=0).sum() + torch.stack(value_losses, dim=0).sum()
                losses.backward()
                
                meta_opt.step()

            all_losses = []
            saved_log_probs = []
            saved_state_values = []

            optimizee = w(target_to_opt(
                **{k: detach_var(v) for k, v in optimizee_params.items()}))

        else:
            optimizee = w(target_to_opt(**optimizee_params))
            assert len(list(optimizee.all_named_parameters()))
            
    return all_losses_ever

def fit_optimizer_AC(target_cls, target_to_opt, unroll=20, optim_it=100, n_epochs=20, n_tests=100, lr=0.001, out_mul=1.0, **kwargs):
    opt_net = w(AC_Optimizer(**kwargs))
    meta_opt = optim.Adam(opt_net.parameters(), lr=lr)

    best_net = None
    best_loss = 1e17

    for _ in tqdm(range(n_epochs), 'epochs'):
        for _ in tqdm(range(20), 'iterations'):
            do_fit_AC(opt_net, meta_opt, target_cls, target_to_opt,
                      unroll, optim_it, out_mul, phase='train')

        loss = (np.mean([
            np.sum(do_fit_AC(opt_net, meta_opt, target_cls, target_to_opt,
                             unroll, optim_it, out_mul, phase='val'))
            for _ in tqdm(range(n_tests), 'tests')
        ]))
        print(loss)
        if loss < best_loss:
            print(best_loss, loss)
            best_loss = loss
            best_net = copy.deepcopy(opt_net.state_dict())

    return best_loss, best_net

"""Experiments

optimizer is supposed to find a 10-element vector called  that, when multiplied by a 10x10 matrix called , is as close as possible to a 10-element vector called . Both  and  are generated randomly. The error is simply the squared error.
"""

class QuadraticLoss:
    def __init__(self, **kwargs):
        self.W = w(Variable(torch.randn(10, 10)))
        self.y = w(Variable(torch.randn(10)))
        
    def get_loss(self, theta):
        return torch.sum((self.W.matmul(theta) - self.y)**2)
    
class QuadOptimizee(nn.Module):
    def __init__(self, theta=None):
        super().__init__()
        # Note: assuming the same optimization for theta as for
        # the function to find out itself.
        if theta is None:
#             self.theta = nn.Parameter(torch.randn(10)/10.0 - 0.05)
            self.theta = nn.Parameter(torch.zeros(10))
        else:
            self.theta = theta
        
    def forward(self, target):
        return target.get_loss(self.theta)
    
    def all_named_parameters(self):
        return [('theta', self.theta)]

class RNN_Optimizer(nn.Module):
    def __init__(self, preproc=True, hidden_sz=20, preproc_factor=10.0):
        super().__init__()
        self.hidden_sz = hidden_sz
        if preproc:
            self.recurs = nn.LSTMCell(2, hidden_sz)
        else:
            self.recurs = nn.LSTMCell(1, hidden_sz)
        self.recurs2 = nn.LSTMCell(hidden_sz, hidden_sz)
        self.output = nn.Linear(hidden_sz, 1)
        self.preproc = preproc
        self.preproc_factor = preproc_factor
        
    def forward(self, inp, hidden, cell):
        if self.preproc:
            # Implement preproc described in Appendix A
            
            # Note: we do all this work on tensors, which means
            # the gradients won't propagate through inp. This
            # should be ok because the algorithm involves
            # making sure that inp is already detached.
            inp = preprocess(inp, self.preproc_factor)
        hidden0, cell0 = self.recurs(inp, (hidden[0], cell[0]))
        hidden1, cell1 = self.recurs2(hidden0, (hidden[1], cell[1]))
        return self.output(hidden1), (hidden0, hidden1), (cell0, cell1)

class PG_Optimizer(nn.Module):
    def __init__(self, preproc=True, hidden_sz=32, n_layers=2, preproc_factor=10.0):
        super().__init__()
        self.n_layers = n_layers
        
        if n_layers == 0:
            if preproc:
                self.linears = nn.ModuleList([nn.Linear(N_PREV_GRADS*2, 1)])
            else:
                self.linears = nn.ModuleList([nn.Linear(N_PREV_GRADS, 1)])
        else:
            if preproc:
                self.linears = nn.ModuleList([nn.Linear(N_PREV_GRADS*2, hidden_sz)])
            else:
                self.linears = nn.ModuleList([nn.Linear(N_PREV_GRADS, hidden_sz)])
            self.linears.extend([nn.Linear(hidden_sz, hidden_sz) for i in range(1, n_layers)])
            self.linears.append(nn.Linear(hidden_sz, 1))
        
        self.preproc = preproc
        self.preproc_factor = preproc_factor

        self.saved_log_probs = []
        self.rewards = []

    def forward(self, inp):
        if self.preproc:
            # Implement preproc described in Appendix A

            # Note: we do all this work on tensors, which means
            # the gradients won't propagate through inp. This
            # should be ok because the algorithm involves
            # making sure that inp is already detached.
            inp = preprocess(inp, self.preproc_factor)
        y = inp
        for i in range(self.n_layers):
            y = self.linears[i](y)
            y = F.relu(y)
        return self.linears[-1](y)

class AC_Optimizer(nn.Module):
    def __init__(self, preproc=True, hidden_sz=32, n_layers=2, preproc_factor=10.0):
        super().__init__()
        self.n_layers = n_layers
        
        if n_layers == 0:
            if preproc:
                self.action_head = nn.Linear(N_PREV_GRADS*2, 1)
                self.value_head = nn.Linear(N_PREV_GRADS*2, 1)

            else:
                self.action_head = nn.Linear(N_PREV_GRADS, 1)
                self.value_head = nn.Linear(N_PREV_GRADS, 1)
            
        else:
            if preproc:
                self.linears = nn.ModuleList([nn.Linear(N_PREV_GRADS*2, hidden_sz)])
            else:
                self.linears = nn.ModuleList([nn.Linear(N_PREV_GRADS, hidden_sz)])
            self.linears.extend([nn.Linear(hidden_sz, hidden_sz) for i in range(1, n_layers)])
            self.action_head = nn.Linear(hidden_sz, 1)
            self.value_head = nn.Linear(hidden_sz, 1)
        
        self.preproc = preproc
        self.preproc_factor = preproc_factor

        self.saved_log_probs = []
        self.rewards = []

    def forward(self, inp):
        if self.preproc:
            # Implement preproc described in Appendix A

            # Note: we do all this work on tensors, which means
            # the gradients won't propagate through inp. This
            # should be ok because the algorithm involves
            # making sure that inp is already detached.
            inp = preprocess(inp, self.preproc_factor)
        y = inp
        for i in range(self.n_layers):
            y = self.linears[i](y)
            y = F.relu(y)
        return self.action_head(y), self.value_head(y)

# for lr in tqdm([1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001, 0.00003, 0.00001], 'all'):
#     print('Trying lr:', lr)
#     print(fit_optimizer(QuadraticLoss, QuadOptimizee, lr=lr)[0])

loss, rnn_quad_optimizer = fit_optimizer(QuadraticLoss, QuadOptimizee, lr=0.003, n_epochs=5, preproc=True)
print(loss)

# for lr in tqdm([1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001, 0.00003, 0.00001], 'all'):
#     print('Trying lr:', lr)
#     print(fit_optimizer_LPG(QuadraticLoss, QuadOptimizee, lr=lr)[0])

loss, linear_quad_optimizer = fit_optimizer_PG(QuadraticLoss, QuadOptimizee, lr=0.003, n_epochs=10, hidden_sz=None, n_layers=0, preproc=True)
print(loss)

# for lr in tqdm([1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001, 0.00003, 0.00001], 'all'):
#     print('Trying lr:', lr)

#     opt_net = PG_Optimizer(hidden_sz=32, n_layers=2, preproc=True)
#     print(fit_optimizer_PG(QuadraticLoss, QuadOptimizee, opt_net, lr=lr)[0])

loss, pg_quad_optimizer = fit_optimizer_PG(QuadraticLoss, QuadOptimizee, lr=0.001, n_epochs=10, hidden_sz=32, n_layers=2, preproc=True)
print(loss)

loss, ac_quad_optimizer = fit_optimizer_AC(QuadraticLoss, QuadOptimizee, lr=0.001, n_epochs=10, hidden_sz=32, n_layers=2, preproc=True)
print(loss)

def fit_normal(target_cls, target_to_opt, opt_class, n_tests=100, n_epochs=100, **kwargs):
    results = []
    for i in tqdm(range(n_tests), 'tests'):
        target = target_cls(phase='test')
        optimizee = w(target_to_opt())
        optimizer = opt_class(optimizee.parameters(), **kwargs)
        total_loss = []
        for _ in range(n_epochs):
            loss = optimizee(target)

            total_loss.append(loss.data.cpu().numpy())
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        results.append(total_loss)
    return results

def find_best_lr_normal(target_cls, target_to_opt, opt_class, **extra_kwargs):
    best_loss = 1000000000000000.0
    best_lr = 0.0
    for lr in tqdm([1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001, 0.00003, 0.00001], 'Learning rates'):
        try:
            loss = best_loss + 1.0
            loss = np.mean([np.sum(s) for s in fit_normal(target_cls, target_to_opt, opt_class, lr=lr, **extra_kwargs)])
        except RuntimeError as e:
            print(e)
            pass
        
        print (loss, lr)
        if loss < best_loss:
            best_loss = loss
            best_lr = lr
    return best_loss, best_lr

NORMAL_OPTS = [(optim.Adam, {}), (optim.RMSprop, {}), (optim.SGD, {}), (optim.SGD, {'nesterov': True, 'momentum': 0.9})]
OPT_NAMES = ['ADAM', 'RMSprop', 'SGD', 'NAG']

# NB: the momentum parameter for nesterov was found from the following file: https://github.com/torch/optim/blob/master/nag.lua
# since it is mentioned in the paper that "When an optimizer has more parameters than just a learning rate (e.g. decay coefficients for ADAM) we use the default values from the optim package in Torch7."
# for opt, kwargs in NORMAL_OPTS:
#     print(find_best_lr_normal(QuadraticLoss, QuadOptimizee, opt, *

QUAD_LRS = [0.1, 0.03, 0.01, 0.01]
NUMBER_OF_TEST_RUNS = 20
NUMBER_OF_EPOCHS = 20

fit_data = np.zeros((NUMBER_OF_TEST_RUNS, NUMBER_OF_EPOCHS, len(OPT_NAMES) + 3))
for i, ((opt, extra_kwargs), lr) in enumerate(zip(NORMAL_OPTS, QUAD_LRS)):
    np.random.seed(0)
    fit_data[:, :, i] = np.array(fit_normal(QuadraticLoss, QuadOptimizee, opt, lr=lr, 
                                            n_tests=NUMBER_OF_TEST_RUNS, n_epochs=NUMBER_OF_EPOCHS, **extra_kwargs))

opt = w(RNN_Optimizer())
opt.load_state_dict(rnn_quad_optimizer)
np.random.seed(0)
fit_data[:, :, len(OPT_NAMES)] = np.array([do_fit(opt, None, QuadraticLoss, QuadOptimizee, 1, NUMBER_OF_EPOCHS, out_mul=1.0, phase='test') for _ in range(NUMBER_OF_TEST_RUNS)])

opt = w(PG_Optimizer(hidden_sz=None, n_layers=0, preproc=True))
opt.load_state_dict(linear_quad_optimizer)
np.random.seed(0)
fit_data[:, :, len(OPT_NAMES) + 1] = np.array([do_fit_PG(opt, None, QuadraticLoss, QuadOptimizee, 
                                                      1, NUMBER_OF_EPOCHS, out_mul=1.0, phase='test') for _ in range(NUMBER_OF_TEST_RUNS)])

opt = w(PG_Optimizer())
opt.load_state_dict(pg_quad_optimizer)
np.random.seed(0)
fit_data[:, :, len(OPT_NAMES) + 2] = np.array([do_fit_PG(opt, None, QuadraticLoss, QuadOptimizee, 1, NUMBER_OF_EPOCHS, out_mul=1.0, phase='test') for _ in range(NUMBER_OF_TEST_RUNS)])

plt.figure(figsize=(16, 8))
ax = sns.lineplot(data=fit_data[:, :100, :], condition=OPT_NAMES + ['RNN', 'Linear Reinforce', 'Policy Gradient'], linestyle='--', color=['r', 'b', 'c', 'g', 'k', 'm', 'y'])
ax.lines[-1].set_linestyle('-')
ax.lines[-2].set_linestyle('-')
ax.lines[-3].set_linestyle('-')
ax.legend()
plt.yscale('log')
plt.xlabel('steps')
plt.ylabel('loss')
plt.title('Quadratic functions')
plt.show()

plt.figure(figsize=(16, 8))
ax = sns.lineplot(data=fit_data[:, :, :], condition=OPT_NAMES + ['RNN', 'Linear Reinforce', 'Policy Gradient'], linestyle='--', color=['r', 'b', 'c', 'g', 'k', 'm', 'y'])
ax.lines[-1].set_linestyle('-')
ax.lines[-2].set_linestyle('-')
ax.lines[-3].set_linestyle('-')
ax.legend()
plt.yscale('log')
plt.xlabel('steps')
plt.ylabel('loss')
plt.title('Quadratic functions')
plt.show()